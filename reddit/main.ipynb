{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal sentences:  10154317\n",
      "Sea/ocean sentences:  29548\n"
     ]
    }
   ],
   "source": [
    "# Use python 3.8\n",
    "# my env: bhuman\n",
    "splits = [\"train\", \"test\", \"dev\"]\n",
    "\n",
    "sentences = []\n",
    "for split in splits:\n",
    "    with open(f\"reddit_conversations.3turns.{split}.topical.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "        sentences.extend(lines)\n",
    "\n",
    "# clean text\n",
    "sentences = [s.strip() for s in sentences]\n",
    "sentences = [\" \".join(s.split(\"\\t\")[:3]) for s in sentences]\n",
    "print(\"Orginal sentences: \", len(sentences)) \n",
    "\n",
    "# ocean text\n",
    "sentences = [s for s in sentences if \"sea\" in s.split() or \"ocean\" in s.split() ] \n",
    "print(\"Sea/ocean sentences: \", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 443731), ('the', 287253), ('like', 246351), ('know', 226494), ('people', 226405), ('think', 225180), ('good', 219017), ('time', 218913), ('actually', 218386), ('right', 217353), ('way', 217242), ('thing', 216840), ('sure', 216152), ('pretty', 215124), ('probably', 213903), ('going', 213762), (',', 213147), ('need', 209633), ('re', 209051), ('ve', 208570), ('a', 207810), ('things', 206689), ('ll', 206521), ('sea', 193476), ('thought', 186981), ('away', 182794), ('i', 179230), ('yeah', 172381), ('?', 168110), ('maybe', 166999), ('got', 160991), ('want', 158945), ('long', 156587), ('you', 152683), ('years', 152079), ('mean', 150422), ('shit', 148576), ('to', 148524), ('ocean', 145574), ('it', 144935), ('makes', 144731), ('url', 141721), ('big', 131062), ('use', 126802), ('work', 124612), ('body', 121725), ('of', 121080), ('cause', 119957), ('is', 119258), ('and', 112870), ('that', 112206), ('little', 112186), ('bad', 109745), ('man', 107760), ('better', 106176), ('look', 105604), ('come', 104219), ('fuck', 104139), ('stuff', 103875), ('cool', 103383), ('in', 102862), ('water', 102213), ('fucking', 101676), ('inside', 99828), ('case', 99017), ('said', 97844), (\"'s\", 92236), ('life', 90863), ('yes', 89060), ('kind', 88588), ('sounds', 86309), ('day', 84497), ('kill', 84261), ('lol', 84126), ('human', 83971), ('point', 82320), ('death', 81925), ('whale', 78716), (\"n't\", 77728), ('fish', 76289), ('live', 74763), ('gets', 73398), ('eat', 71258), ('fact', 70455), ('food', 70448), ('humans', 68027), ('die', 67298), ('imagine', 66938), ('spiders', 66726), ('eating', 66548), ('shark', 66043), ('dead', 65938), ('sharks', 65808), ('species', 65622), ('giant', 65174), ('bugs', 65009), ('bacteria', 64946), ('mouth', 64941), ('bite', 64928), ('snakes', 64922)]\n"
     ]
    }
   ],
   "source": [
    "# compute the frequency of each word using Counter\n",
    "# from collections import Counter\n",
    "# word_freq = Counter()\n",
    "# for s in sentences:\n",
    "#     word_freq.update(s.split())\n",
    "# # print top 100\n",
    "# print(word_freq.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the top 100 words to a csv file\n",
    "# import csv\n",
    "# with open(\"top100.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow([\"word\", \"freq\"])\n",
    "#     for word, freq in word_freq.most_common(100):\n",
    "#         writer.writerow([word, freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-31 10:06:23,978 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29548/29548 [00:33<00:00, 891.11it/s] \n",
      "Batch inference: 100%|██████████| 77/77 [01:21<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# perform pos tagging with flair on sentences\n",
    "import flair\n",
    "import torch\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "from tqdm import tqdm\n",
    "flair.device = torch.device('cuda:0')\n",
    "tagger = SequenceTagger.load(\"pos-fast\")\n",
    "flair_sent = [Sentence(s) for s in tqdm(sentences)]\n",
    "tagger.predict(flair_sent, mini_batch_size=384, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sea', 19681), ('ocean', 15384), ('water', 3613), ('people', 2673), ('time', 1669), ('salt', 1590), ('fish', 1556), ('land', 1525), ('world', 1497), ('thing', 1229), ('level', 1203), ('something', 1185), ('man', 1160), ('way', 1110), ('one', 1091), ('life', 1048), ('url', 954), ('day', 945), ('s', 918), ('china', 845), ('movie', 823), ('years', 821), ('earth', 810), ('lot', 798), ('bottom', 796), ('m', 784), ('someone', 778), ('country', 772), ('today', 742), ('things', 684), ('fact', 684), ('boat', 676), ('point', 672), ('reddit', 662), ('part', 651), ('island', 617), ('food', 611), ('space', 605), ('song', 593), ('%', 572), ('beach', 570), ('middle', 566), ('air', 565), ('guy', 558), ('name', 557), ('place', 545), ('anything', 542), ('year', 530), ('ice', 527), ('everyone', 509), ('ship', 498), ('lake', 490), ('nothing', 488), ('lol', 488), ('city', 475), ('oceans', 474), ('war', 474), ('sharks', 455), ('anyone', 447), ('north', 443), ('thanks', 439), ('person', 438), ('coast', 432), ('word', 423), ('trump', 421), ('question', 414), ('idea', 408), ('joke', 402), ('sky', 400), ('bit', 393), ('russia', 388), ('humans', 385), ('problem', 380), ('miles', 379), ('oil', 378), ('guys', 356), ('game', 355), ('animal', 354), ('lion', 353), ('floor', 352), ('planet', 351), ('everything', 348), ('florida', 347), ('side', 345), ('shark', 345), ('state', 343), ('animals', 337), ('stuff', 337), ('ships', 335), ('wall', 333), ('feet', 332), ('navy', 331), ('shells', 326), ('america', 324), ('countries', 323), ('whale', 320), ('night', 319), ('reason', 319), ('border', 318), ('levels', 317)]\n",
      "[('good', 1803), ('other', 1467), ('more', 1456), ('big', 1218), ('great', 988), ('deep', 951), ('same', 940), ('sure', 882), ('blue', 864), ('new', 817), ('old', 756), ('most', 742), ('bad', 717), ('many', 716), ('best', 672), ('pacific', 659), ('first', 647), ('much', 645), ('little', 636), ('better', 618), ('right', 604), ('real', 599), ('few', 588), ('last', 583), ('true', 560), ('dead', 558), ('high', 489), ('atlantic', 486), ('nice', 482), ('red', 476), ('whole', 466), ('favorite', 457), ('black', 450), ('only', 444), ('wrong', 426), ('frank', 417), ('south', 409), ('least', 398), ('white', 387), ('small', 380), ('different', 372), ('own', 357), ('entire', 352), ('giant', 336), ('long', 327), ('salty', 323), ('cool', 316), ('free', 314), ('amazing', 300), ('next', 294), ('open', 293), ('full', 288), ('fun', 283), ('awesome', 278), ('huge', 273), ('weird', 267), ('hard', 265), ('large', 261), ('worst', 255), ('american', 255), ('enough', 254), ('fine', 252), ('funny', 246), ('beautiful', 244), ('cold', 240), ('less', 234), ('global', 228), ('nuclear', 223), ('stupid', 223), ('interesting', 222), ('able', 217), ('single', 211), ('close', 210), ('sad', 206), ('super', 204), ('fresh', 204), ('wet', 203), ('common', 201), ('serious', 198), ('fair', 198), ('indian', 196), ('biggest', 194), ('happy', 192), ('human', 192), ('specific', 188), ('worse', 187), ('worth', 186), ('actual', 184), ('dark', 183), ('angry', 183), ('easy', 183), ('mediterranean', 182), ('homemade', 178), ('hot', 176), ('marine', 169), ('dry', 169), ('crazy', 167), ('expensive', 163), ('higher', 163), ('chinese', 160)]\n"
     ]
    }
   ],
   "source": [
    "noun_tags = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "adj_tags = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "\n",
    "# [Flair package] select and count all nouns in flair_sent using Counter\n",
    "noun_freq = Counter()\n",
    "for s in flair_sent:\n",
    "    for token in s:\n",
    "        if token.tag in noun_tags:\n",
    "            noun_freq.update([token.text])\n",
    "print(noun_freq.most_common(100))\n",
    "\n",
    "# [Flair package] selelct and count all adjs in flair_sent using Counter\n",
    "adj_freq = Counter()\n",
    "for s in flair_sent:\n",
    "    for token in s:\n",
    "        if token.tag in adj_tags:\n",
    "            adj_freq.update([token.text])\n",
    "print(adj_freq.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# noun_freq = [('water', 3613), ('people', 2673), ('time', 1669), ('salt', 1590), ('fish', 1556), ('land', 1525), ('world', 1497), ('thing', 1229), ('level', 1203), ('something', 1185), ('man', 1160), ('way', 1110), ('one', 1091), ('life', 1048), ('url', 954), ('day', 945), ('s', 918), ('china', 845), ('movie', 823), ('years', 821), ('earth', 810), ('lot', 798), ('bottom', 796), ('m', 784), ('someone', 778), ('country', 772), ('today', 742), ('things', 684), ('fact', 684), ('boat', 676), ('point', 672), ('reddit', 662), ('part', 651), ('island', 617), ('food', 611), ('space', 605), ('song', 593), ('%', 572), ('beach', 570), ('middle', 566), ('air', 565), ('guy', 558), ('name', 557), ('place', 545), ('anything', 542), ('year', 530), ('ice', 527), ('everyone', 509), ('ship', 498), ('lake', 490), ('nothing', 488), ('lol', 488), ('city', 475), ('oceans', 474), ('war', 474), ('sharks', 455), ('anyone', 447), ('north', 443), ('thanks', 439), ('person', 438), ('coast', 432), ('word', 423), ('trump', 421), ('question', 414), ('idea', 408), ('joke', 402), ('sky', 400), ('bit', 393), ('russia', 388), ('humans', 385), ('problem', 380), ('miles', 379), ('oil', 378), ('guys', 356), ('game', 355), ('animal', 354), ('lion', 353), ('floor', 352), ('planet', 351), ('everything', 348), ('florida', 347), ('side', 345), ('shark', 345), ('state', 343), ('animals', 337), ('stuff', 337), ('ships', 335), ('wall', 333), ('feet', 332), ('navy', 331), ('shells', 326), ('america', 324), ('countries', 323), ('whale', 320), ('night', 319), ('reason', 319), ('border', 318), ('levels', 317)]\n",
    "# adj_freq = [('good', 1803), ('other', 1467), ('more', 1456), ('big', 1218), ('great', 988), ('deep', 951), ('same', 940), ('sure', 882), ('blue', 864), ('new', 817), ('old', 756), ('most', 742), ('bad', 717), ('many', 716), ('best', 672), ('pacific', 659), ('first', 647), ('much', 645), ('little', 636), ('better', 618), ('right', 604), ('real', 599), ('few', 588), ('last', 583), ('true', 560), ('dead', 558), ('high', 489), ('atlantic', 486), ('nice', 482), ('red', 476), ('whole', 466), ('favorite', 457), ('black', 450), ('only', 444), ('wrong', 426), ('frank', 417), ('south', 409), ('least', 398), ('white', 387), ('small', 380), ('different', 372), ('own', 357), ('entire', 352), ('giant', 336), ('long', 327), ('salty', 323), ('cool', 316), ('free', 314), ('amazing', 300), ('next', 294), ('open', 293), ('full', 288), ('fun', 283), ('awesome', 278), ('huge', 273), ('weird', 267), ('hard', 265), ('large', 261), ('worst', 255), ('american', 255), ('enough', 254), ('fine', 252), ('funny', 246), ('beautiful', 244), ('cold', 240), ('less', 234), ('global', 228), ('nuclear', 223), ('stupid', 223), ('interesting', 222), ('able', 217), ('single', 211), ('close', 210), ('sad', 206), ('super', 204), ('fresh', 204), ('wet', 203), ('common', 201), ('serious', 198), ('fair', 198), ('indian', 196), ('biggest', 194), ('happy', 192), ('human', 192), ('specific', 188), ('worse', 187), ('worth', 186), ('actual', 184), ('dark', 183), ('angry', 183), ('easy', 183), ('mediterranean', 182), ('homemade', 178), ('hot', 176), ('marine', 169), ('dry', 169), ('crazy', 167), ('expensive', 163), ('higher', 163), ('chinese', 160)]\n",
    "# to csv, two columns (word, freq)\n",
    "with open('noun_freq.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"word\", \"freq\"])\n",
    "    writer.writerows(noun_freq)\n",
    "\n",
    "with open('adj_freq.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"word\", \"freq\"])\n",
    "    writer.writerows(adj_freq)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
